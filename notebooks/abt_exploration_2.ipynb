{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from names_dataset import NameDataset\n",
    "from tqdm import tqdm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Note\n",
    "Token labels are presented in BIO (Beginning, Inner, Outer) format. The PII type is prefixed with “B-” when it is the beginning of an entity. If the token is a continuation of an entity, it is prefixed with “I-”. Tokens that are not PII are labeled “O”.\n",
    "\n",
    "**Model idea** Lets instead of using the predefined token space, we define a new one without BIO definition, but where we concat tokens that are part of the same entity. We can then afterwards use SpaCy tokenizer to reverse the changes we made and get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/train.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[True, True, True, False, False, True, False, ...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Assignment:  Visualization Reflection  Submitt...</td>\n",
       "      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
       "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
       "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "2  [True, False, False, True, True, False, False,...   \n",
       "3  [True, True, True, False, False, True, False, ...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "O                   4989794\n",
       "B-NAME_STUDENT         1365\n",
       "I-NAME_STUDENT         1096\n",
       "B-URL_PERSONAL          110\n",
       "B-ID_NUM                 78\n",
       "B-EMAIL                  39\n",
       "I-STREET_ADDRESS         20\n",
       "I-PHONE_NUM              15\n",
       "B-USERNAME                6\n",
       "B-PHONE_NUM               6\n",
       "B-STREET_ADDRESS          2\n",
       "I-URL_PERSONAL            1\n",
       "I-ID_NUM                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.labels.explode().reset_index(drop=True)\n",
    "labels_vc = labels.value_counts()\n",
    "labels_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[True, True, True, False, False, True, False, ...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Assignment:  Visualization Reflection  Submitt...</td>\n",
       "      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
       "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
       "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "2  [True, False, False, True, True, False, False,...   \n",
       "3  [True, True, True, False, False, True, False, ...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "O                   4989794\n",
       "B-NAME_STUDENT         1365\n",
       "I-NAME_STUDENT         1096\n",
       "B-URL_PERSONAL          110\n",
       "B-ID_NUM                 78\n",
       "B-EMAIL                  39\n",
       "I-STREET_ADDRESS         20\n",
       "I-PHONE_NUM              15\n",
       "B-USERNAME                6\n",
       "B-PHONE_NUM               6\n",
       "B-STREET_ADDRESS          2\n",
       "I-URL_PERSONAL            1\n",
       "I-ID_NUM                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_exploded = df.tokens.explode().reset_index(drop=True)\n",
    "labels_exploded = df.labels.explode().reset_index(drop=True)\n",
    "labels_exploded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and next time I want to concentrate on them , too \n",
      "\n",
      " Waseem Mabunda   591 Smith Centers Apt . 656 \n",
      " Joshuamouth , RI 95963 ( The Netherlands )\n",
      "--------------------------------------------------\n",
      "the business model , and got ourselves an   identity ( 1861 . Milano - 743 Erika Bypass Apt . 419 \n",
      " Andreahaven , IL 54207 ) looking forward to\n"
     ]
    }
   ],
   "source": [
    "ids = labels_exploded[labels_exploded == 'B-STREET_ADDRESS'].index\n",
    "print(\" \".join(tokens_exploded[ids[0]-15:ids[0]+15]))\n",
    "print(\"-\"*50)\n",
    "print(\" \".join(tokens_exploded[ids[1]-15:ids[1]+15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861200           Smith\n",
       "861201         Centers\n",
       "861202             Apt\n",
       "861203               .\n",
       "861204             656\n",
       "861205              \\n\n",
       "861206     Joshuamouth\n",
       "861207               ,\n",
       "861208              RI\n",
       "861209           95963\n",
       "1445331          Erika\n",
       "1445332         Bypass\n",
       "1445333            Apt\n",
       "1445334              .\n",
       "1445335            419\n",
       "1445336             \\n\n",
       "1445337    Andreahaven\n",
       "1445338              ,\n",
       "1445339             IL\n",
       "1445340          54207\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = labels_exploded[labels_exploded == 'I-STREET_ADDRESS'].index\n",
    "tokens_exploded[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "We will attempt to create the simplest model possible. The model will evaluate each token and pass it through a couple functions to evaluate it. If one of the functions returns true, the token will be considered a keyword. The model will then return the keyword and the index of the token in the input string.\n",
    "\n",
    "### Model Functions\n",
    "- **email** This function will use regex to identify if the token is an email address.\n",
    "- **phone_num** This function will use regex to identify if the token is a phone number.\n",
    "- **address** This function will use regex to identify if the token is an address.\n",
    "- **username** This function will use a word embedding model to identify if the token is a common word and if not then we flag it as a username.\n",
    "- **personal_id** This function will use regex to identify if the token is a personal id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = NameDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do**\n",
    "Maybe we could make some more features for capitalised words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.first_names.get(\"Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    # Remove excessive line breaks or spaces\n",
    "    text = re.sub('(\\r\\n){2,}', ' ', text)\n",
    "    \n",
    "    # Convert the text to lowercase and split into tokens\n",
    "    tokens = text.lower().split(\" \")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "class PiiModel:\n",
    "    def __init__(self, text: str = None, names_dataset = None, tokens: list = None, significant_tokens: dict = None, dataframe: pd.DataFrame = None) -> None:\n",
    "        self.text = text\n",
    "        self.names = names_dataset\n",
    "        self.tokens = tokens\n",
    "        self.tokenizer = tokenizer\n",
    "        self.name_rank_threhsold = 1500\n",
    "        self.country_threshold = 0.01\n",
    "        self.significant_tokens = significant_tokens\n",
    "        self.feature_df = pd.DataFrame()\n",
    "        self.input_df = dataframe\n",
    "        pass\n",
    "    \n",
    "    def get_firstname(self, name: str) -> bool:\n",
    "        found_name = self.names.first_names.get(name.capitalize())\n",
    "        # we check if US exists found_name.country\n",
    "        if not found_name:\n",
    "            return False\n",
    "        country = found_name[\"country\"]\n",
    "        if country.get(\"US\", 0) > self.country_threshold and found_name[\"rank\"].get(\"US\", 1e6) < self.name_rank_threhsold:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def get_lastname(self, lastname: str) -> bool:\n",
    "        found_name = self.names.last_names.get(lastname.capitalize())\n",
    "        # we check if the max country is higher than a threshold then we say it is a valid name\n",
    "        if not found_name:\n",
    "            return False\n",
    "        country = found_name[\"country\"]\n",
    "        if country.get(\"US\", 0) > self.country_threshold and found_name[\"rank\"].get(\"US\", 1e6) < self.name_rank_threhsold:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_email_regex(self):\n",
    "        return r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "    \n",
    "    def get_phone_number_regex(self):\n",
    "        return r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n",
    "    \n",
    "    def get_address_regex(self):\n",
    "        return r'\\d{1,5}\\s\\w.\\s(\\b\\w*\\b\\s){1,2}\\w*\\.'\n",
    "    \n",
    "    def get_personal_id_regex(self):\n",
    "        return r'\\d{6,12}\\b|\\b[a-zA-Z]+\\d{2,}'\n",
    "    \n",
    "    def get_url_personal_regex(self):\n",
    "        return r'https?://[\\w\\.-]+'\n",
    "    \n",
    "    def numb_percent(self, text: str) -> float:\n",
    "        return sum(c.isdigit() for c in text) / len(text)\n",
    "    \n",
    "    def word_length(self, text: str) -> int:\n",
    "        return len(text)\n",
    "    \n",
    "    def is_punctuationmark(self, text: str) -> bool:\n",
    "        return text in [\".\", \",\", \"!\", \"?\", \";\", \":\"]\n",
    "    \n",
    "    def stop_words(self, text: str) -> bool:\n",
    "        return text in nlp.Defaults.stop_words\n",
    "    \n",
    "    def NER_ingest(self, text: str):\n",
    "        return nlp(text)\n",
    "    \n",
    "    def capitalize(self, text: str) -> bool:\n",
    "        return text[0].strip().isupper()\n",
    "    \n",
    "    def contextual_regex(self, tokens: list, regex: str) -> bool:\n",
    "        \"\"\" This function will attempt to incorporate the contextual information of the tokens to the regex.\n",
    "        Essentially, we want to consider the tokens before and after the token to determine if the token is a match.\n",
    "        Firstly, we check if the previous token is was detected as a match, if so, we add it to the current context, we do this until \n",
    "        we reach the beginning of the text or we find a token that was not detected as a match.\n",
    "        Then, we move forward n steps and progressively add tokens and check if the regex matches the context.\n",
    "        In the end, we return a boolean indicating if the regex was found in the context.\n",
    "\n",
    "        Args:\n",
    "            regex (str): The regex to be matched\n",
    "\n",
    "        Returns:\n",
    "            bool: A boolean indicating if the regex was found in the context\n",
    "        \"\"\"\n",
    "\n",
    "        current_context = \"\"\n",
    "        col_classifications = [False for _ in range(len(tokens))]\n",
    "        n_forward = 3 # We move forward 3 steps\n",
    "\n",
    "        # We commence a sliding window approach to check the context of the tokens\n",
    "        for i in range(len(tokens)):\n",
    "            # We iteratively move backwards until we reach the beginning of the text or we find a token that was not detected as a match\n",
    "            while i >= 0 and col_classifications[i] == True:\n",
    "                current_context = tokens[i] + \" \" + current_context\n",
    "                i -= 1\n",
    "            # We move forward n steps and progressively add tokens and check if the regex matches the context\n",
    "            for j in range(n_forward):\n",
    "                if i + j < len(tokens):\n",
    "                    current_context = current_context + \" \" + tokens[i + j]\n",
    "                    if re.match(regex, current_context):\n",
    "                        col_classifications[i + j] = True\n",
    "            current_context = \"\"\n",
    "        return col_classifications\n",
    "\n",
    "    \n",
    "    def check_preceding_contextual_tokens(self, index, label):\n",
    "        # Extract relevant tokens directly, avoiding repeated DataFrame row access\n",
    "        start_index = max(0, index-2)\n",
    "        tokens_to_check = self.feature_df['token'][start_index:index]\n",
    "        \n",
    "        # Get the set of significant tokens for the label, defaulting to an empty set if not found\n",
    "        significant_tokens_set = self.significant_tokens.get(label, set())\n",
    "        \n",
    "        # Check if any of the tokens to check are in the significant tokens set\n",
    "        return any(token in significant_tokens_set for token in tokens_to_check)\n",
    "\n",
    "    \n",
    "    def build_df(self, text: str = \"\", tokens: list = None, labels: list = None) -> pd.DataFrame:\n",
    "        token_indices = self.tokens\n",
    "        if not self.tokens and text != \"\":\n",
    "            token_indices = self.tokenizer(text)\n",
    "            self.tokens = token_indices\n",
    "        if tokens:\n",
    "            token_indices = tokens\n",
    "        elif self.input_df is not None:\n",
    "            token_indices = self.input_df[\"tokens\"]\n",
    "        self.feature_df[\"token\"] = token_indices\n",
    "\n",
    "        self.feature_df['labels'] = labels\n",
    "        return None\n",
    "\n",
    "    def build_features(self) -> list[tuple[int, str]]:\n",
    "        # Computing Features\n",
    "        self.feature_df['first_name'] = self.feature_df['token'].apply(lambda x: self.get_firstname(x))\n",
    "        self.feature_df['last_name'] = self.feature_df['token'].apply(lambda x: self.get_lastname(x))\n",
    "        self.feature_df['email'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_email_regex(), x)))\n",
    "        self.feature_df['r_neighboured_email'] = self.feature_df['email'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_email'] = self.feature_df['email'].shift(1).fillna(False)\n",
    "        self.feature_df['phone_number'] = self.contextual_regex(self.feature_df['token'], self.get_phone_number_regex())\n",
    "        self.feature_df['r_neighboured_phone_number'] = self.feature_df['phone_number'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_phone_number'] = self.feature_df['phone_number'].shift(1).fillna(False)\n",
    "        self.feature_df['address'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_address_regex(), x)))\n",
    "        self.feature_df['r_neighboured_address'] = self.feature_df['address'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_address'] = self.feature_df['address'].shift(1).fillna(False)\n",
    "        self.feature_df['personal_id'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_personal_id_regex(), x)))\n",
    "        self.feature_df['r_neighboured_personal_id'] = self.feature_df['personal_id'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_personal_id'] = self.feature_df['personal_id'].shift(1).fillna(False)\n",
    "        self.feature_df['url'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_url_personal_regex(), x)))\n",
    "        self.feature_df['r_neighboured_url'] = self.feature_df['url'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_url'] = self.feature_df['url'].shift(1).fillna(False)\n",
    "        self.feature_df['capitalized'] = self.feature_df['token'].apply(lambda x: self.capitalize(x))\n",
    "        self.feature_df['r_neighboured_capitalized'] = self.feature_df['capitalized'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_capitalized'] = self.feature_df['capitalized'].shift(1).fillna(False)\n",
    "        self.feature_df['numb_percent'] = self.feature_df['token'].apply(self.numb_percent)\n",
    "        self.feature_df['r_neighboured_numb_percent'] = self.feature_df['numb_percent'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_numb_percent'] = self.feature_df['numb_percent'].shift(1).fillna(False)\n",
    "        self.feature_df['word_length'] = self.feature_df['token'].apply(self.word_length)\n",
    "        self.feature_df['norm_index'] = self.feature_df.index / len(self.feature_df)\n",
    "        self.feature_df['is_punctuation'] = self.feature_df['token'].apply(self.is_punctuationmark)\n",
    "        self.feature_df['r_neighboured_punctuation'] = self.feature_df['is_punctuation'].shift(-1).fillna(False)\n",
    "        self.feature_df['l_neighboured_punctuation'] = self.feature_df['is_punctuation'].shift(1).fillna(False)\n",
    "        self.feature_df['stop_word'] = self.feature_df['token'].apply(self.stop_words)\n",
    "        if self.input_df is not None:\n",
    "            self.feature_df[\"text_index\"] = self.input_df.text_index\n",
    "        # Add a new feature for each label based on contextual tokens\n",
    "        # for label in self.significant_tokens.keys():\n",
    "        #     feature_name = f'{label.lower()}_contextual_presence'\n",
    "        #     self.feature_df[feature_name] = [self.check_preceding_contextual_tokens(i, label) for i in range(len(self.feature_df))]\n",
    "\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.tokens, df.labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4560 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4560/4560 [01:06<00:00, 68.29it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs_analysis = []\n",
    "\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    tokens, labels = X_train.iloc[i], y_train.iloc[i]\n",
    "    model = PiiModel(text=tokens, names_dataset=names)\n",
    "    model.build_df(tokens=tokens, labels=labels)\n",
    "    model.build_features()\n",
    "    \n",
    "    # Reset index and drop the old index to ensure unique indices\n",
    "    model.feature_df.reset_index(drop=True, inplace=True)\n",
    "    # drop duplicate columns\n",
    "    model.feature_df = model.feature_df.loc[:,~model.feature_df.columns.duplicated()]\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs_analysis.append(model.feature_df)\n",
    "\n",
    "X_analysis = pd.concat(dfs_analysis, axis=0)\n",
    "X_analysis.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2247/2247 [00:35<00:00, 63.44it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs_test = []\n",
    "\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    tokens, labels = X_test.iloc[i], y_test.iloc[i]\n",
    "    model = PiiModel(text=tokens, names_dataset=names)\n",
    "    model.build_df(tokens=tokens, labels=labels)\n",
    "    model.build_features()\n",
    "    \n",
    "    # Reset index and drop the old index to ensure unique indices\n",
    "    model.feature_df.reset_index(drop=True, inplace=True)\n",
    "    # drop duplicate columns\n",
    "    model.feature_df = model.feature_df.loc[:,~model.feature_df.columns.duplicated()]\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs_test.append(model.feature_df)\n",
    "\n",
    "X_test = pd.concat(dfs_test, axis=0)\n",
    "X_test.fillna(0, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
