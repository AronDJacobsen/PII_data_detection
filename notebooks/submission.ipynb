{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from names_dataset import NameDataset\n",
    "from tqdm import tqdm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "ner_model = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Note\n",
    "Token labels are presented in BIO (Beginning, Inner, Outer) format. The PII type is prefixed with “B-” when it is the beginning of an entity. If the token is a continuation of an entity, it is prefixed with “I-”. Tokens that are not PII are labeled “O”.\n",
    "\n",
    "**Model idea** Lets instead of using the predefined token space, we define a new one without BIO definition, but where we concat tokens that are part of the same entity. We can then afterwards use SpaCy tokenizer to reverse the changes we made and get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/train.json\"\n",
    "file_path_test = \"../data/test.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open(file_path_test, \"r\") as file:\n",
    "    data_test = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_test = pd.DataFrame(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.labels.explode().reset_index(drop=True)\n",
    "labels_vc = labels.value_counts()\n",
    "labels_vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "We will attempt to create the simplest model possible. The model will evaluate each token and pass it through a couple functions to evaluate it. If one of the functions returns true, the token will be considered a keyword. The model will then return the keyword and the index of the token in the input string.\n",
    "\n",
    "### Model Functions\n",
    "- **email** This function will use regex to identify if the token is an email address.\n",
    "- **phone_num** This function will use regex to identify if the token is a phone number.\n",
    "- **address** This function will use regex to identify if the token is an address.\n",
    "- **username** This function will use a word embedding model to identify if the token is a common word and if not then we flag it as a username.\n",
    "- **personal_id** This function will use regex to identify if the token is a personal id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = NameDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do**\n",
    "Maybe we could make some more features for capitalised words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    # Remove excessive line breaks or spaces\n",
    "    text = re.sub('(\\r\\n){2,}', ' ', text)\n",
    "    \n",
    "    # Convert the text to lowercase and split into tokens\n",
    "    tokens = text.lower().split(\" \")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "class PiiModel:\n",
    "    def __init__(self, text: str = None, \n",
    "                 names_dataset = None, tokens: list = None, \n",
    "                 significant_tokens: dict = None, \n",
    "                 dataframe: pd.DataFrame = None, full_text: str = None):\n",
    "        self.text = text\n",
    "        self.names = names_dataset\n",
    "        self.tokens = tokens\n",
    "        self.tokenizer = tokenizer\n",
    "        self.name_rank_threhsold = 1500\n",
    "        self.country_threshold = 0.01\n",
    "        self.significant_tokens = significant_tokens\n",
    "        self.feature_df = pd.DataFrame()\n",
    "        self.input_df = dataframe\n",
    "        self.full_text = full_text\n",
    "        pass\n",
    "    \n",
    "    def get_firstname(self, name: str) -> bool:\n",
    "        found_name = self.names.first_names.get(name.capitalize())\n",
    "\n",
    "        if not found_name:\n",
    "            return False\n",
    "        country = found_name[\"country\"]\n",
    "        # If any of the countries has a higher percentage than the threshold and the rank is lower than the threshold\n",
    "        if any(value > self.country_threshold for value in country.values()) and any(rank < self.name_rank_threhsold for rank in found_name[\"rank\"].values()):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def get_lastname(self, lastname: str) -> bool:\n",
    "        found_name = self.names.last_names.get(lastname.capitalize())\n",
    "        # we check if the max country is higher than a threshold then we say it is a valid name\n",
    "        if not found_name:\n",
    "            return False\n",
    "        country = found_name[\"country\"]\n",
    "        if any(value > self.country_threshold for value in country.values()) and any(rank < self.name_rank_threhsold for rank in found_name[\"rank\"].values()):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_email_regex(self):\n",
    "        return r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "    \n",
    "    def get_phone_number_regex(self):\n",
    "        return r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n",
    "    \n",
    "    def get_address_regex(self):\n",
    "        return r'\\d{1,5}\\s\\w.\\s(\\b\\w*\\b\\s){1,2}\\w*\\.'\n",
    "    \n",
    "    def get_personal_id_regex(self):\n",
    "        return r'\\d{6,12}\\b|\\b[a-zA-Z]+\\d{2,}'\n",
    "    \n",
    "    def get_url_personal_regex(self):\n",
    "        return r'https?://[\\w\\.-]+'\n",
    "    \n",
    "    def numb_percent(self, text: str) -> float:\n",
    "        return sum(c.isdigit() for c in text) / len(text)\n",
    "    \n",
    "    def word_length(self, text: str) -> int:\n",
    "        return len(text)\n",
    "    \n",
    "    def is_punctuationmark(self, text: str) -> bool:\n",
    "        return text in [\".\", \",\", \"!\", \"?\", \";\", \":\"]\n",
    "    \n",
    "    def stop_words(self, text: str) -> bool:\n",
    "        return text in nlp.Defaults.stop_words\n",
    "    \n",
    "    def capitalize(self, text: str) -> bool:\n",
    "        return text[0].strip().isupper()\n",
    "    \n",
    "    def NER_ingest(self, text: str):\n",
    "        return ner_model(text)\n",
    "\n",
    "    def contextual_ner_person(self, text: str) -> bool:\n",
    "        return text in list(self.NER_persons)\n",
    "    \n",
    "    def contextual_ner_location(self, text: str) -> bool:\n",
    "        return text in (self.NER_locations)\n",
    "    \n",
    "    def check_preceding_contextual_tokens(self, index, label):\n",
    "        # Extract relevant tokens directly, avoiding repeated DataFrame row access\n",
    "        start_index = max(0, index-2)\n",
    "        tokens_to_check = self.feature_df['token'][start_index:index]\n",
    "        \n",
    "        # Get the set of significant tokens for the label, defaulting to an empty set if not found\n",
    "        significant_tokens_set = self.significant_tokens.get(label, set())\n",
    "        \n",
    "        # Check if any of the tokens to check are in the significant tokens set\n",
    "        return any(token in significant_tokens_set for token in tokens_to_check)\n",
    "\n",
    "    \n",
    "    def build_df(self, text: str = \"\", tokens: list = None, labels: list = None) -> pd.DataFrame:\n",
    "        token_indices = self.tokens\n",
    "        if not self.tokens and text != \"\":\n",
    "            token_indices = self.tokenizer(text)\n",
    "            self.tokens = token_indices\n",
    "        if tokens:\n",
    "            token_indices = tokens\n",
    "        elif self.input_df is not None:\n",
    "            token_indices = self.input_df[\"tokens\"]\n",
    "        self.feature_df[\"token\"] = token_indices\n",
    "\n",
    "        self.feature_df['labels'] = labels\n",
    "        return None\n",
    "\n",
    "    def build_features(self) -> list[tuple[int, str]]:\n",
    "        # Computing Features\n",
    "        self.feature_df['first_name'] = self.feature_df['token'].apply(self.get_firstname)\n",
    "        self.feature_df['last_name'] = self.feature_df['token'].apply(self.get_lastname)\n",
    "        self.feature_df['email'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_email_regex(), x)))\n",
    "        self.feature_df['phone_number'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_phone_number_regex(), x)))\n",
    "        self.feature_df['address'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_address_regex(), x)))\n",
    "        self.feature_df['personal_id'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_personal_id_regex(), x)))\n",
    "        self.feature_df['url'] = self.feature_df['token'].apply(lambda x: bool(re.match(self.get_url_personal_regex(), x)))\n",
    "        self.feature_df['capitalized'] = self.feature_df['token'].apply(lambda x: self.capitalize(x))\n",
    "        self.feature_df['numb_percent'] = self.feature_df['token'].apply(self.numb_percent)\n",
    "        self.feature_df['word_length'] = self.feature_df['token'].apply(self.word_length)\n",
    "        self.feature_df['norm_index'] = self.feature_df.index / len(self.feature_df)\n",
    "        self.feature_df['is_punctuation'] = self.feature_df['token'].apply(self.is_punctuationmark)\n",
    "        self.feature_df['stop_word'] = self.feature_df['token'].apply(self.stop_words)\n",
    "\n",
    "        ingested = self.NER_ingest(self.full_text)\n",
    "\n",
    "        self.NER_persons = pd.Series([entity['word'] for entity in ingested if entity['entity_group'] == 'PER']).explode().reset_index(drop=True)\n",
    "        self.NER_locations = pd.Series([entity['word'] for entity in ingested if entity['entity_group'] == 'LOC']).explode().reset_index(drop=True)\n",
    "\n",
    "        self.feature_df['contextual_ner_person'] = self.feature_df['token'].apply(self.contextual_ner_person)\n",
    "        self.feature_df['contextual_ner_location'] = self.feature_df['token'].apply(self.contextual_ner_location)\n",
    "\n",
    "        # Window\n",
    "        for i in range(1, 6):\n",
    "            self.feature_df[f'r_neighboured_email_{i}'] = self.feature_df['email'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_email_{i}'] = self.feature_df['email'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_phone_number_{i}'] = self.feature_df['phone_number'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_phone_number_{i}'] = self.feature_df['phone_number'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_address_{i}'] = self.feature_df['address'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_address_{i}'] = self.feature_df['address'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_personal_id_{i}'] = self.feature_df['personal_id'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_personal_id_{i}'] = self.feature_df['personal_id'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_url_{i}'] = self.feature_df['url'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_url_{i}'] = self.feature_df['url'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_capitalized_{i}'] = self.feature_df['capitalized'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_capitalized_{i}'] = self.feature_df['capitalized'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_numb_percent_{i}'] = self.feature_df['numb_percent'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_numb_percent_{i}'] = self.feature_df['numb_percent'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_punctuation_{i}'] = self.feature_df['is_punctuation'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_punctuation_{i}'] = self.feature_df['is_punctuation'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_stop_word_{i}'] = self.feature_df['stop_word'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_stop_word_{i}'] = self.feature_df['stop_word'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_word_length_{i}'] = self.feature_df['word_length'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_word_length_{i}'] = self.feature_df['word_length'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_first_name_{i}'] = self.feature_df['first_name'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_first_name_{i}'] = self.feature_df['first_name'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_last_name_{i}'] = self.feature_df['last_name'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_last_name_{i}'] = self.feature_df['last_name'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_contextual_ner_person_{i}'] = self.feature_df['contextual_ner_person'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_contextual_ner_person_{i}'] = self.feature_df['contextual_ner_person'].shift(i).fillna(False)\n",
    "            self.feature_df[f'r_neighboured_contextual_ner_location_{i}'] = self.feature_df['contextual_ner_location'].shift(-i).fillna(False)\n",
    "            self.feature_df[f'l_neighboured_contextual_ner_location_{i}'] = self.feature_df['contextual_ner_location'].shift(i).fillna(False)\n",
    "\n",
    "\n",
    "        if self.input_df is not None:\n",
    "            self.feature_df[\"text_index\"] = self.input_df.text_index\n",
    "        # Add a new feature for each label based on contextual tokens\n",
    "        for label in self.significant_tokens.keys():\n",
    "            feature_name = f'{label.lower()}_contextual_presence'\n",
    "            self.feature_df[feature_name] = [self.check_preceding_contextual_tokens(i, label) for i in range(len(self.feature_df))]\n",
    "\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preceeding Words analysis on Xtrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_analysis = []\n",
    "\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    tokens, labels = X_train.tokens.iloc[i], y_train.iloc[i]\n",
    "    model = PiiModel(text=tokens, names_dataset=names)\n",
    "    model.build_df(tokens=tokens, labels=labels)\n",
    "    \n",
    "    # Reset index and drop the old index to ensure unique indices\n",
    "    model.feature_df.reset_index(drop=True, inplace=True)\n",
    "    # drop duplicate columns\n",
    "    model.feature_df = model.feature_df.loc[:,~model.feature_df.columns.duplicated()]\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs_analysis.append(model.feature_df)\n",
    "\n",
    "X_analysis = pd.concat(dfs_analysis, axis=0)\n",
    "X_analysis.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_analysis_df = X_analysis[['token', 'labels']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_analysis_df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preceding_tokens(row_index, N, dataframe):\n",
    "    # Calculate the start index to avoid negative indexing\n",
    "    start_index = max(0, row_index - N)\n",
    "    # Extract preceding N tokens, if the row_index is not at the very beginning\n",
    "    if start_index < row_index:\n",
    "        return dataframe.iloc[start_index:row_index]['token'].tolist()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "N = 2  # Example: Look 5 tokens back\n",
    "labels_of_interest = ['B-NAME_STUDENT', 'I-NAME_STUDENT', 'B-ID_NUM',\n",
    "       'B-URL_PERSONAL', 'B-EMAIL', 'B-USERNAME', 'I-URL_PERSONAL',\n",
    "       'I-ID_NUM', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS']\n",
    "\n",
    "# Initialize a dictionary to hold token frequencies for each label\n",
    "token_frequencies_by_label = {label: {} for label in labels_of_interest}\n",
    "\n",
    "for label in labels_of_interest:\n",
    "    # Filter rows for the specific label\n",
    "    label_indices = _analysis_df[_analysis_df['labels'] == label].index\n",
    "    \n",
    "    # Collect preceding tokens for each occurrence of the label\n",
    "    for index in label_indices:\n",
    "        preceding_tokens = get_preceding_tokens(index, N, _analysis_df)\n",
    "        for token in preceding_tokens:\n",
    "            if token in token_frequencies_by_label[label]:\n",
    "                token_frequencies_by_label[label][token] += 1\n",
    "            else:\n",
    "                token_frequencies_by_label[label][token] = 1\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# This dictionary will count the occurrences of each token across labels\n",
    "token_occurrences_across_labels = defaultdict(int)\n",
    "\n",
    "# Count occurrences of each token across different labels\n",
    "for label, frequencies in token_frequencies_by_label.items():\n",
    "    for token in frequencies.keys():\n",
    "        token_occurrences_across_labels[token] += 1\n",
    "\n",
    "# Identify tokens that appear in more than one label's frequency dictionary\n",
    "common_tokens = {token for token, count in token_occurrences_across_labels.items() if count > 1}\n",
    "\n",
    "\n",
    "# Remove common tokens from each label's frequency dictionary\n",
    "filtered_token_frequencies_by_label = {\n",
    "    label: {token: frequency for token, frequency in frequencies.items() if token not in common_tokens}\n",
    "    for label, frequencies in token_frequencies_by_label.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = []\n",
    "\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    tokens, labels = df.tokens.iloc[i], df.iloc[i]\n",
    "    model = PiiModel(text=tokens, names_dataset=names, significant_tokens=token_frequencies_by_label, full_text=df.full_text.iloc[i])\n",
    "    model.build_df(tokens=tokens, labels=labels)\n",
    "    model.build_features()\n",
    "    \n",
    "    # Reset index and drop the old index to ensure unique indices\n",
    "    model.feature_df.reset_index(drop=True, inplace=True)\n",
    "    # drop duplicate columns\n",
    "    model.feature_df = model.feature_df.loc[:,~model.feature_df.columns.duplicated()]\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs_train.append(model.feature_df)\n",
    "\n",
    "X_train = pd.concat(dfs_train, axis=0)\n",
    "X_train.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_test = []\n",
    "\n",
    "for i in tqdm(range(df_test.shape[0])):\n",
    "    tokens, labels = df_test.tokens.iloc[i], df_test.iloc[i]\n",
    "    model = PiiModel(text=tokens, names_dataset=names, significant_tokens=token_frequencies_by_label, full_text=df_test.full_text.iloc[i])\n",
    "    model.build_df(tokens=tokens, labels=labels)\n",
    "    model.build_features()\n",
    "    \n",
    "    # Reset index and drop the old index to ensure unique indices\n",
    "    model.feature_df.reset_index(drop=True, inplace=True)\n",
    "    # drop duplicate columns\n",
    "    model.feature_df = model.feature_df.loc[:,~model.feature_df.columns.duplicated()]\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs_test.append(model.feature_df)\n",
    "\n",
    "X_test = pd.concat(dfs_test, axis=0)\n",
    "X_test.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique entries in y_test\n",
    "X_test['labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "We will use the XGBoost model to classify the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "y_train = X_train['labels']\n",
    "y_test = X_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "model.fit(X_train.drop(columns=[\"token\", \"labels\"]), y_train.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test.drop(columns=[\"token\", \"labels\"]), y_test.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test.drop(columns=[\"token\", \"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\"predictions\": pd.Series(predictions).reset_index(drop=True), \n",
    "                           \"labels\": y_test.explode().reset_index(drop=True), \n",
    "                           \"tokens\": X_test.token.explode().reset_index(drop=True)})\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = df_results[\n",
    "    ((df_results['predictions'] != \"O\") | (df_results['labels'] != \"O\")) & \n",
    "    (df_results['predictions'] == df_results['labels'])\n",
    "].shape[0] / df_results[(df_results['predictions'] != \"O\") | (df_results['labels'] != \"O\")].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_score(df_results['labels'], df_results['predictions'], average='micro', beta=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
